# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jAN2no_EAhOSHniYN4joLGXvvYTyYwgm
"""

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, Bidirectional
from sklearn.preprocessing import MinMaxScaler

# Load the data from Yahoo Finance (for a stock symbol, e.g., 'AAPL' for Apple)
stock_symbol = 'AAPL'  # Change this to your stock symbol or use your dataset
data = yf.download(stock_symbol, start='2010-01-01', end='2020-04-01')

# Display the first few rows of the data
print(data.head())

# Visualize the Closing Prices
plt.figure(figsize=(10, 6))
plt.plot(data['Close'], label='Close Price')
plt.title(f'{stock_symbol} Stock Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# --- EDA (Exploratory Data Analysis) ---
import seaborn as sns
import matplotlib.pyplot as plt

# Display basic information about the dataset
print(data.info())

# Display the first few rows
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Plot the distribution of the stock's closing price
plt.figure(figsize=(10, 6))
sns.histplot(data['Close'], bins=50, kde=True)
plt.title(f'{stock_symbol} Closing Price Distribution')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Plot daily returns (percentage change)
data['Returns'] = data['Close'].pct_change() * 100  # Percentage change in closing price
plt.figure(figsize=(10, 6))
plt.plot(data['Returns'], label='Daily Returns')
plt.title(f'{stock_symbol} Daily Returns')
plt.xlabel('Date')
plt.ylabel('Returns (%)')
plt.legend()
plt.show()

# Visualize the correlation matrix of numerical features
correlation_matrix = data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title(f'{stock_symbol} Feature Correlation Matrix')
plt.show()

# Check for seasonality by plotting rolling mean and rolling std of the closing price
rolling_mean = data['Close'].rolling(window=30).mean()
rolling_std = data['Close'].rolling(window=30).std()

plt.figure(figsize=(10, 6))
plt.plot(data['Close'], label='Close Price')
plt.plot(rolling_mean, label='Rolling Mean (30 days)', color='orange')
plt.plot(rolling_std, label='Rolling Std (30 days)', color='red')
plt.title(f'{stock_symbol} Closing Price with Rolling Mean and Std')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# Preprocess the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))

# Create training and testing datasets
train_size = int(len(scaled_data) * 0.80)
train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]

# Create sequences efficiently
def create_sequences(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
        X.append(dataset[i:i + look_back, 0])
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 60
X_train, Y_train = create_sequences(train_data, look_back)
X_test, Y_test = create_sequences(test_data, look_back)

# Reshape the data for LSTM input in one step
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Build the bi-LSTM model
model = Sequential()
model.add(Bidirectional(LSTM(units=50, return_sequences=True, input_shape=(1, look_back))))
model.add(LSTM(units=50))
model.add(Dense(1))

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_data=(X_test, Y_test))

# Plot the training and validation loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Invert the predictions and true values back to original scale
train_predict = scaler.inverse_transform(train_predict)
Y_train = scaler.inverse_transform(Y_train.reshape(-1, 1))
test_predict = scaler.inverse_transform(test_predict)
Y_test = scaler.inverse_transform(Y_test.reshape(-1, 1))

# Calculate performance metrics for training data
train_rmse = np.sqrt(mean_squared_error(Y_train, train_predict))
train_mae = mean_absolute_error(Y_train, train_predict)
train_r2 = r2_score(Y_train, train_predict)

# Calculate performance metrics for testing data
test_rmse = np.sqrt(mean_squared_error(Y_test, test_predict))
test_mae = mean_absolute_error(Y_test, test_predict)
test_r2 = r2_score(Y_test, test_predict)

# Print the performance metrics
print("Training Performance Metrics:")
print(f"RMSE: {train_rmse:.4f}")
print(f"MAE: {train_mae:.4f}")
print(f"R² Score: {train_r2:.4f}")

print("\nTesting Performance Metrics:")
print(f"RMSE: {test_rmse:.4f}")
print(f"MAE: {test_mae:.4f}")
print(f"R² Score: {test_r2:.4f}")

# Plot the train vs. predicted
plt.figure(figsize=(10, 6))
plt.plot(Y_train, label='True')
plt.plot(train_predict, label='Predicted')
plt.title('Train vs. Predicted')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

# Plot the test vs. predicted
plt.figure(figsize=(10, 6))
plt.plot(Y_test, label='True')
plt.plot(test_predict, label='Predicted')
plt.title('Test vs. Predicted')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()